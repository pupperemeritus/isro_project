{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pupperemeritus/miniconda3/envs/isro-project/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Optional, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import torch\n",
    "from darts import TimeSeries\n",
    "from darts.metrics import mae, mape, smape\n",
    "from darts.models import BlockRNNModel\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import (EarlyStopping, LearningRateMonitor,\n",
    "                                         ModelCheckpoint)\n",
    "from pytorch_lightning.loggers import MLFlowLogger\n",
    "import torch.nn.functional as F\n",
    "from model.data_loader import IonosphereDataModule, load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough points for interpolation. Using binning instead. Points: 1\n",
      "Not enough points for interpolation. Using binning instead. Points: 1\n",
      "Not enough points for interpolation. Using binning instead. Points: 1\n",
      "Not enough points for interpolation. Using binning instead. Points: 1\n",
      "Not enough points for interpolation. Using binning instead. Points: 1\n",
      "Not enough points for interpolation. Using binning instead. Points: 1\n",
      "Not enough points for interpolation. Using binning instead. Points: 1\n",
      "Not enough points for interpolation. Using binning instead. Points: 1\n",
      "Not enough points for interpolation. Using binning instead. Points: 1\n",
      "Not enough points for interpolation. Using binning instead. Points: 1\n",
      "Not enough points for interpolation. Using binning instead. Points: 1\n",
      "Not enough points for interpolation. Using binning instead. Points: 1\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "data = load_data(\"/home/pupperemeritus/isro_project/data/October2023.parquet\")\n",
    "\n",
    "# Clear CUDA cache\n",
    "torch.clear_autocast_cache()\n",
    "torch.cuda.memory.empty_cache()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Define parameters\n",
    "prediction_horizon = 30\n",
    "grid_resolution = 5\n",
    "time_window = \"1m\"\n",
    "stride = 15\n",
    "\n",
    "# Create data module\n",
    "data_module = IonosphereDataModule(\n",
    "    dataframe=data,\n",
    "    sequence_length=30,\n",
    "    prediction_horizon=prediction_horizon,\n",
    "    grid_lat_range=(0, 40),\n",
    "    grid_lon_range=(65, 100),\n",
    "    grid_resolution=grid_resolution,\n",
    "    time_window=time_window,\n",
    "    stride=stride,\n",
    "    batch_size=32,\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "# Prepare data\n",
    "data_module.setup()\n",
    "train_loader = data_module.train_dataloader()\n",
    "val_loader = data_module.val_dataloader()\n",
    "test_loader = data_module.test_dataloader()\n",
    "\n",
    "\n",
    "def pad_to_max_length(batch_list):\n",
    "    max_length = max([tensor.shape[0] for tensor in batch_list])\n",
    "    padded_batch_list = [\n",
    "        F.pad(tensor, (0, 0, 0, max_length - tensor.shape[0])) for tensor in batch_list\n",
    "    ]\n",
    "    return padded_batch_list\n",
    "\n",
    "\n",
    "def loader_to_timeseries(loader):\n",
    "    all_data_s4 = []\n",
    "    all_data_phase = []\n",
    "    for batch in loader:\n",
    "        # Combine features and targets for S4\n",
    "        s4_data = torch.cat([batch[\"features_s4\"], batch[\"target_s4\"]], dim=0)\n",
    "        s4_data = s4_data.reshape(s4_data.shape[0], -1)  # Flatten spatial dimensions\n",
    "        all_data_s4.append(s4_data)\n",
    "\n",
    "        # Combine features and targets for phase\n",
    "        phase_data = torch.cat([batch[\"features_phase\"], batch[\"target_phase\"]], dim=0)\n",
    "        phase_data = phase_data.reshape(\n",
    "            phase_data.shape[0], -1\n",
    "        )  # Flatten spatial dimensions\n",
    "        all_data_phase.append(phase_data)\n",
    "\n",
    "    # Pad sequences to the same length\n",
    "    all_data_s4 = torch.stack(pad_to_max_length(all_data_s4), dim=0)\n",
    "    all_data_phase = torch.stack(pad_to_max_length(all_data_phase), dim=0)\n",
    "\n",
    "    # Create a multivariate TimeSeries\n",
    "    return TimeSeries.from_tensor(\n",
    "        torch.stack([all_data_s4, all_data_phase], dim=-1), columns=[\"S4\", \"Phase\"]\n",
    "    )\n",
    "\n",
    "\n",
    "train_series = loader_to_timeseries(train_loader)\n",
    "val_series = loader_to_timeseries(val_loader)\n",
    "test_series = loader_to_timeseries(test_loader)\n",
    "\n",
    "# Define Darts LSTM model\n",
    "model = BlockRNNModel(\n",
    "    model=\"LSTM\",\n",
    "    input_chunk_length=30,\n",
    "    output_chunk_length=prediction_horizon,\n",
    "    n_rnn_layers=2,\n",
    "    hidden_dim=128,\n",
    "    batch_size=32,\n",
    "    n_epochs=50,\n",
    "    optimizer_kwargs={\"lr\": 0.001},\n",
    "    model_name=\"DartsLSTMForecast\",\n",
    "    pl_trainer_kwargs={\n",
    "        \"accelerator\": \"gpu\",\n",
    "        \"devices\": 1,\n",
    "        \"callbacks\": [\n",
    "            EarlyStopping(monitor=\"val_loss\", patience=15, mode=\"min\"),\n",
    "            ModelCheckpoint(\n",
    "                dirpath=\"checkpoints\",\n",
    "                filename=\"darts-lstm-{epoch:02d}-{val_loss:.5f}\",\n",
    "                save_top_k=3,\n",
    "                monitor=\"val_loss\",\n",
    "                mode=\"min\",\n",
    "            ),\n",
    "            LearningRateMonitor(logging_interval=\"epoch\"),\n",
    "        ],\n",
    "        \"logger\": MLFlowLogger(\n",
    "            experiment_name=\"darts_lstm_logs\", tracking_uri=\"mlruns\"\n",
    "        ),\n",
    "        \"precision\": \"16\",\n",
    "        \"enable_progress_bar\": True,\n",
    "        \"accumulate_grad_batches\": 3,\n",
    "        \"profiler\": \"simple\",\n",
    "        \"min_epochs\": 25,\n",
    "        \"deterministic\": True,\n",
    "    },\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_series, val_series=val_series)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(n=len(test_series), series=test_series)\n",
    "\n",
    "# Evaluate the model\n",
    "mape_score = mape(test_series, predictions)\n",
    "smape_score = smape(test_series, predictions)\n",
    "mae_score = mae(test_series, predictions)\n",
    "\n",
    "print(f\"MAPE: {mape_score}\")\n",
    "print(f\"SMAPE: {smape_score}\")\n",
    "print(f\"MAE: {mae_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isro-project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
